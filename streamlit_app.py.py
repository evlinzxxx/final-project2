# -*- coding: utf-8 -*-
"""Copy of jaya-institute.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fwke9wMEtKNEoTGcxE86F_pwf2HlkM_K
"""

import os
import joblib
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import plotly.express as px
import matplotlib.pyplot as plt
from joblib import dump, load

from sklearn import metrics
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from scipy.stats import chi2_contingency
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.svm import LinearSVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score

# set maximum display column
pd.set_option('display.max_columns', None)

# read csv into dataframe
df = pd.read_csv('/content/drive/MyDrive/Dataset/dropout.csv',delimiter=';')

# show information columns
df.info()

# show 5 first data
df.head()

# calculate missing value
df.isna().sum()

"""tidak ada data yang mengalami missing value atau nilai kosong"""

# calculate duplicated data
df.duplicated().sum()

"""tidak ada juga data yang ganda atau terduplikasi"""

# show describe
df.describe()

# Menghitung jumlah student berdasarkan status
df.Status.value_counts()

# Pisahkan student yang berstatus Enrolled, untuk selanjutnya di prediksi meggunakan model machine learning
df_enrolled = df[df.Status=='Enrolled']
df = df[~(df.Status=='Enrolled')]

# Menghitung jumlah setiap status
status_counts = df['Status'].value_counts()

# Warna yang lebih bervariasi dan eksplosi
colors = ['#76c7c0', '#ff6f61']
explode = [0.1, 0]  # Membuat slice pertama (misalnya, status pertama) sedikit meledak keluar

# Membuat pie chart
plt.figure(figsize=(10, 8))
plt.pie(
    status_counts,
    labels=status_counts.index,
    autopct='%1.1f%%',
    colors=colors,
    startangle=140,
    explode=explode,
    shadow=True,
    wedgeprops={'edgecolor': 'black'}
)
plt.title('Status Distribution', fontsize=14, fontweight='bold')
plt.show()

"""Insight:  
- **Student graduate** mendominasi lebih dari setengah dengan angka mencapai **60.9%**, menunjukkan mayoritas mahasiswa berhasil menyelesaikan studi mereka.  
- **Student dropout** mencapai **39.1%**, yang mengindikasikan hampir 4 dari 10 mahasiswa tidak menyelesaikan pendidikan mereka.  
- Perbandingan ini menunjukkan adanya tantangan yang signifikan dalam mempertahankan mahasiswa hingga lulus, sehingga dapat menjadi fokus analisis lebih lanjut untuk mengidentifikasi faktor-faktor penyebab dropout dan strategi untuk meningkatkannya.
"""

categorical_columns = [
    'Marital_status', 'Application_mode', 'Course',
    'Daytime_evening_attendance', 'Previous_qualification',
    'Nacionality','Mothers_qualification', 'Fathers_qualification',
    'Mothers_occupation', 'Fathers_occupation','Displaced',
    'Educational_special_needs', 'Debtor', 'Tuition_fees_up_to_date',
    'Gender', 'Scholarship_holder', 'International'
]

colors = {'Graduate': '#3498db', 'Dropout': '#e74c3c'}

# Jumlah kolom untuk grid subplot
num_cols = 3
num_rows = -(-len(numeric_columns) // num_cols)  # Pembagian pembulatan ke atas

# Membuat subplots
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(18, 6 * num_rows))
axes = axes.flatten()

# Loop melalui setiap kolom numerik
for i, column in enumerate(numeric_columns):
    ax = axes[i]
    for status in df['Status'].unique():
        subset = df[df['Status'] == status]
        ax.hist(subset[column], bins=15, alpha=0.7, label=status, color=colors[status])
    ax.set_xlabel(column, fontsize=12)
    ax.set_ylabel('Frequency', fontsize=12)
    ax.set_title(f'Distribution of {column}', fontsize=14, fontweight='bold')
    ax.legend(title='Status', fontsize=10)
    ax.grid(axis='y', linestyle='--', alpha=0.6)

# Hapus subplot kosong
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

# Membuat palet warna kustom
colors = {'Graduate': '#3498db', 'Dropout': '#e74c3c'}

# Create subplots
fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 6 * num_rows))

# Flatten the axes array for easy iteration
axes = axes.flatten()

# Loop melalui setiap kolom kategorikal
for i, column in enumerate(categorical_columns):
    for status in df['Status'].unique():
        subset = df[df['Status'] == status]
        ax = axes[i]
        sns.countplot(x=column, data=subset, hue='Status',alpha=0.5, palette=colors, ax=ax)
        ax.set_xlabel(column)
        ax.set_ylabel('Count')
        ax.set_title(f'{column}')
        ax.legend(title='Status')

# Remove empty subplots if any
for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

"""## Data Preparation / Preprocessing"""

# Mengganti nilai 'Graduate' dengan 0 dan 'Dropout' dengan 1 dalam kolom 'Status'
df['Status'] = df['Status'].replace({'Graduate': 0, 'Dropout': 1}).astype(int)


# Menghitung matriks korelasi
correlation_matrix = df.drop(columns=categorical_columns).corr()

# Membuat heatmap dengan penyesuaian tambahan
plt.figure(figsize=(16, 9))
sns.heatmap(
    correlation_matrix,
    annot=True,
    cmap='viridis',
    fmt=".2f",
    linewidths=0.5,
    cbar_kws={'shrink': 0.8, 'label': 'Correlation Coefficient'},
    annot_kws={'size': 10}
)
plt.title('Correlation Heatmap of Numeric Features', fontsize=16)
plt.xticks(rotation=90, fontsize=10)
plt.yticks(fontsize=10)
plt.tight_layout()
plt.show()

# Menghapus kolom kolom yang tidak akan digunakan
deleted_columns =["Unemployment_rate",
                  "GDP", "Inflation_rate", "Curricular_units_2nd_sem_evaluations",
                  "Curricular_units_2nd_sem_without_evaluations",
                  "Curricular_units_1st_sem_without_evaluations",
                  "Curricular_units_1st_sem_evaluations",
                  "Educational_special_needs","International", "Course", "Application_mode",
                  "Nacionality", "Mothers_qualification", "Fathers_qualification",
                  "Mothers_occupation", "Fathers_occupation", 'Marital_status','Age_at_enrollment',
                  "Previous_qualification"
]

df_train = df.drop(columns=deleted_columns)
df_train.info()

# Split data into train and test sets
X = df_train.drop('Status', axis=1)
y = df_train['Status']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define numerical features
numerical_features = ['Application_order', 'Previous_qualification_grade', 'Admission_grade',
                      'Curricular_units_1st_sem_enrolled', 'Curricular_units_2nd_sem_enrolled',
                      'Curricular_units_1st_sem_credited', 'Curricular_units_2nd_sem_credited',
                      'Curricular_units_1st_sem_approved', 'Curricular_units_2nd_sem_approved',
                      'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade']

# Preprocessing pipeline
preprocessor = ColumnTransformer(
    [('scaler', StandardScaler(), numerical_features)],
    remainder='passthrough'
)

# Define Random Forest Classifier
rf_classifier = RandomForestClassifier(random_state=42)

# Combine preprocessing and classifier into a pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', rf_classifier)
])

# Set hyperparameter grid for GridSearchCV
param_grid = {
    'classifier__n_estimators': [100, 200, 300],
    'classifier__max_depth': [None, 10, 20, 30],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__bootstrap': [True, False]
}

# Perform GridSearchCV
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=5,
    n_jobs=-1,
    verbose=2
)

# Fit the model
grid_search.fit(X_train, y_train)

# Retrieve best model and make predictions
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Evaluate performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

# Save the model and preprocessor
import joblib
joblib.dump(best_model.named_steps['classifier'], 'best_trained_model.pkl')
joblib.dump(best_model.named_steps['preprocessor'].named_transformers_['scaler'], 'best_standard_scaler.pkl')

# Generate confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

# Plot confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=best_model.named_steps['classifier'].classes_, yticklabels=best_model.named_steps['classifier'].classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# Load model and encoder
model = load('best_trained_model.pkl')
standard_scaler = load('best_standard_scaler.pkl')

# Menampilkan feature importance
feature_importances = model.feature_importances_
features = X.columns

# Membuat DataFrame untuk feature importance
importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})

# Mengurutkan berdasarkan importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Menampilkan plot
plt.figure(figsize=(10, 10))
plt.barh(importance_df['Feature'], importance_df['Importance'])
plt.xlabel('Feature Importance')
plt.ylabel('Features')
plt.title('Feature Importance in Random Forest')
plt.gca().invert_yaxis()
plt.show()

# ambil sepuluh fitur penting
# Mengurutkan berdasarkan importance dan ambil 10 fitur penting pertama
top_10_features = importance_df.nlargest(10, 'Importance')

# Menampilkan 10 fitur penting
print(top_10_features)

# Hitung matriks korelasi untuk fitur-fitur penting
correlation_matrix = df_train[top_10_features['Feature']].corr()

# Visualisasikan matriks korelasi menggunakan heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Important Features')
plt.show()

df_enrolled = df_enrolled.reset_index(drop=True)

# Define categorical and numerical features
numerical_features = ['Application_order', 'Previous_qualification_grade', 'Admission_grade',
                      'Curricular_units_1st_sem_enrolled', 'Curricular_units_2nd_sem_enrolled',
                      'Curricular_units_1st_sem_credited', 'Curricular_units_2nd_sem_credited',
                      'Curricular_units_1st_sem_approved', 'Curricular_units_2nd_sem_approved',
                      'Curricular_units_1st_sem_grade', 'Curricular_units_2nd_sem_grade']

# Create preprocessing pipeline
preprocessor = ColumnTransformer(
    transformers=[
        ('scaler', standard_scaler, numerical_features)
    ],
    remainder='passthrough'
)

# Add the preprocessor and model to the pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('classifier', model)
])

# Fit preprocessor to df_enrolled (assuming this is necessary for standard scaler)
pipeline.named_steps['preprocessor'].fit(df_enrolled.drop(deleted_columns + ['Status'], axis=1))

# Preprocess and predict using df_enrolled
y_pred_test = pipeline.predict(df_enrolled.drop(deleted_columns + ['Status'], axis=1))

# Get true labels from df_enrolled
df_enrolled['Status'] = y_pred_test

df_enrolled.Status.value_counts()

# Concatenate df_train and df_enrolled to create df_final
df_final = pd.concat([df_train, df_enrolled.drop(deleted_columns,axis=1)], ignore_index=True)

# Verify the shape of df_final to ensure the concatenation was successful
print("Shape of df_final:", df_final.shape)

# Optionally, save df_final to a CSV file
df_final.to_csv('df_final.csv', index=False)
